{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from torch_geometric.datasets import ModelNet\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "dataset_dir = 'DATA\\modelnet10'\n",
    "\n",
    "pre_transform = T.Compose([\n",
    "    T.SamplePoints(1024, remove_faces=True, include_normals=True),\n",
    "    T.NormalizeScale(),\n",
    "])\n",
    "\n",
    "train_dataset = ModelNet(dataset_dir, name=\"10\", train=True, transform=None, pre_transform=pre_transform, pre_filter=None)\n",
    "test_dataset = ModelNet(dataset_dir, name=\"10\", train=False, transform=None, pre_transform=pre_transform, pre_filter=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset len: 3991\n",
      "Data(pos=[1024, 3], y=[1], normal=[1024, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"train_dataset len:\", len(train_dataset))\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 3])\n",
      "tensor([[ 0.4388, -0.3298, -0.1523],\n",
      "        [-0.3928,  0.1197,  0.0161],\n",
      "        [-0.3305, -0.5762,  0.5107],\n",
      "        ...,\n",
      "        [-0.5001,  0.9977, -0.0072],\n",
      "        [ 0.3320, -0.1300, -0.1173],\n",
      "        [ 0.3466,  0.1909,  0.0216]])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0].pos.shape)\n",
    "print(train_dataset[0].pos)\n",
    "print(train_dataset[0].batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(pos=[16384, 3], y=[16], normal=[16384, 3], batch=[16384], ptr=[17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pei17\\Anaconda3\\envs\\pytorch1.12\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader as DataLoader\n",
    "dataloader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "batch = next(iter(dataloader))\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(pos=[16384, 3], y=[16], normal=[16384, 3], batch=[16384], ptr=[17])\n",
      "torch.Size([16, 512])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import global_max_pool\n",
    "import torch.nn as nn\n",
    "\n",
    "class SymmFunction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SymmFunction, self).__init__()\n",
    "        self.shared_mlp = nn.Sequential(\n",
    "            nn.Linear(3, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
    "            nn.Linear(64, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, 512),\n",
    "        )\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        x = self.shared_mlp(batch.pos)\n",
    "        x = global_max_pool(x, batch.batch)\n",
    "        return x\n",
    "\n",
    "f = SymmFunction()\n",
    "print(batch)\n",
    "y = f(batch)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InputTNet, self).__init__()\n",
    "        self.input_mlp = nn.Sequential(\n",
    "            nn.Linear(3, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
    "            nn.Linear(64, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, 1024), nn.BatchNorm1d(1024), nn.ReLU(),\n",
    "        )\n",
    "        self.output_mlp = nn.Sequential(\n",
    "            nn.Linear(1024, 512), nn.BatchNorm1d(512), nn.ReLU(),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(),\n",
    "            nn.Linear(256, 9)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, batch):\n",
    "        x = self.input_mlp(x)\n",
    "        x = global_max_pool(x, batch)\n",
    "        x = self.output_mlp(x)\n",
    "        x = x.view(-1, 3, 3)\n",
    "        id_matrix = torch.eye(3).to(x.device).view(1, 3, 3).repeat(x.shape[0], 1, 1)\n",
    "        x = id_matrix + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureTNet, self).__init__()\n",
    "        self.input_mlp = nn.Sequential(\n",
    "            nn.Linear(64, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
    "            nn.Linear(64, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, 1024), nn.BatchNorm1d(1024), nn.ReLU(),\n",
    "        )\n",
    "        self.output_mlp = nn.Sequential(\n",
    "            nn.Linear(1024, 512), nn.BatchNorm1d(512), nn.ReLU(),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(),\n",
    "            nn.Linear(256, 64*64)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, batch):\n",
    "        x = self.input_mlp(x)\n",
    "        x = global_max_pool(x, batch)\n",
    "        x = self.output_mlp(x)\n",
    "        x = x.view(-1, 64, 64)\n",
    "        id_matrix = torch.eye(64).to(x.device).view(1, 64, 64).repeat(x.shape[0], 1, 1)\n",
    "        x = id_matrix + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNetClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PointNetClassification, self).__init__()\n",
    "        self.input_tnet = InputTNet()\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(3, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
    "            nn.Linear(64, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
    "        )\n",
    "        self.feature_tnet = FeatureTNet()\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(64, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
    "            nn.Linear(64, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, 1024), nn.BatchNorm1d(1024), nn.ReLU(),\n",
    "        )\n",
    "        self.mlp3 = nn.Sequential(\n",
    "            nn.Linear(1024, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(p=0.3),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(p=0.3),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, batch_data):\n",
    "        x = batch_data.pos\n",
    "        \n",
    "        input_transform = self.input_tnet(x, batch_data.batch)\n",
    "        transform = input_transform[batch_data.batch, :, :]\n",
    "        x = torch.bmm(transform, x.view(-1, 3, 1)).view(-1, 3)\n",
    "        \n",
    "        x = self.mlp1(x)\n",
    "        \n",
    "        feature_transform = self.feature_tnet(x, batch_data.batch)\n",
    "        transform = feature_transform[batch_data.batch, :, :]\n",
    "        x = torch.bmm(transform, x.view(-1, 64, 1)).view(-1, 64)\n",
    "\n",
    "        x = self.mlp2(x)        \n",
    "        x = global_max_pool(x, batch_data.batch)\n",
    "        x = self.mlp3(x)\n",
    "        \n",
    "        return x, input_transform, feature_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "num_epoch = 400\n",
    "batch_size = 32\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = PointNetClassification()\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(lr=1e-4, params=model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=num_epoch // 4, gamma=0.5)\n",
    "\n",
    "#log_dir = current_path / \"log_modelnet10_classification\"\n",
    "#log_dir.mkdir(exist_ok=True)\n",
    "#writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "criteria = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(pos=[32768, 3], y=[32], normal=[32768, 3], batch=[32768], ptr=[33])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 121/125 [00:39<00:01,  3.10it/s]\n",
      "  0%|          | 0/400 [00:39<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [30], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     31\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> 32\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     33\u001b[0m     scheduler\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     35\u001b[0m \u001b[39m#if (epoch % 10 == 0):\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[39m#model_path = log_dir / f\"model_{epoch:06}.pth\"\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     \u001b[39m#torch.save(model.state_dict(), model_path)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pei17\\Anaconda3\\envs\\pytorch1.12\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     64\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\pei17\\Anaconda3\\envs\\pytorch1.12\\lib\\site-packages\\torch\\optim\\optimizer.py:109\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    107\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 109\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\pei17\\Anaconda3\\envs\\pytorch1.12\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\pei17\\Anaconda3\\envs\\pytorch1.12\\lib\\site-packages\\torch\\optim\\adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    155\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 157\u001b[0m     adam(params_with_grad,\n\u001b[0;32m    158\u001b[0m          grads,\n\u001b[0;32m    159\u001b[0m          exp_avgs,\n\u001b[0;32m    160\u001b[0m          exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m          max_exp_avg_sqs,\n\u001b[0;32m    162\u001b[0m          state_steps,\n\u001b[0;32m    163\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    164\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    165\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    166\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    167\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    168\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    169\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    170\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    171\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\pei17\\Anaconda3\\envs\\pytorch1.12\\lib\\site-packages\\torch\\optim\\adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 213\u001b[0m func(params,\n\u001b[0;32m    214\u001b[0m      grads,\n\u001b[0;32m    215\u001b[0m      exp_avgs,\n\u001b[0;32m    216\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    217\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    218\u001b[0m      state_steps,\n\u001b[0;32m    219\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    220\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    221\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    222\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    223\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    224\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    225\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    226\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[1;32mc:\\Users\\pei17\\Anaconda3\\envs\\pytorch1.12\\lib\\site-packages\\torch\\optim\\adam.py:258\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m step_t\u001b[39m.\u001b[39mis_cuda, \u001b[39m\"\u001b[39m\u001b[39mIf capturable=False, state_steps should not be CUDA tensors.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    257\u001b[0m \u001b[39m# update step\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m step_t \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    260\u001b[0m \u001b[39mif\u001b[39;00m weight_decay \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    261\u001b[0m     grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39madd(param, alpha\u001b[39m=\u001b[39mweight_decay)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    model = model.train()\n",
    "    \n",
    "    losses = []\n",
    "    for batch_data in tqdm(train_dataloader, total=len(train_dataloader)):\n",
    "        batch_data = batch_data.to(device)\n",
    "        this_batch_size = batch_data.batch.detach().max() + 1\n",
    "        \n",
    "        pred_y, _, feature_transform = model(batch_data)\n",
    "        true_y = batch_data.y.detach()\n",
    "\n",
    "        class_loss = criteria(pred_y, true_y)\n",
    "        accuracy = float((pred_y.argmax(dim=1) == true_y).sum()) / float(this_batch_size)\n",
    "\n",
    "        id_matrix = torch.eye(feature_transform.shape[1]).to(feature_transform.device).view(1, 64, 64).repeat(feature_transform.shape[0], 1, 1)\n",
    "        transform_norm = torch.norm(torch.bmm(feature_transform, feature_transform.transpose(1, 2)) - id_matrix, dim=(1, 2))\n",
    "        reg_loss = transform_norm.mean()\n",
    "\n",
    "        loss = class_loss + reg_loss * 0.001\n",
    "        \n",
    "        losses.append({\n",
    "            \"loss\": loss.item(),\n",
    "            \"class_loss\": class_loss.item(),\n",
    "            \"reg_loss\": reg_loss.item(),\n",
    "            \"accuracy\": accuracy,\n",
    "            \"seen\": float(this_batch_size)})\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    #if (epoch % 10 == 0):\n",
    "        #model_path = log_dir / f\"model_{epoch:06}.pth\"\n",
    "        #torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    loss = 0\n",
    "    class_loss = 0\n",
    "    reg_loss = 0\n",
    "    accuracy = 0\n",
    "    seen = 0\n",
    "    for d in losses:\n",
    "        seen = seen + d[\"seen\"]\n",
    "        loss = loss + d[\"loss\"] * d[\"seen\"]\n",
    "        class_loss = class_loss + d[\"class_loss\"] * d[\"seen\"]\n",
    "        reg_loss = reg_loss + d[\"reg_loss\"] * d[\"seen\"]\n",
    "        accuracy = accuracy + d[\"accuracy\"] * d[\"seen\"]\n",
    "    loss = loss / seen\n",
    "    class_loss = class_loss / seen\n",
    "    reg_loss = reg_loss / seen\n",
    "    accuracy = accuracy / seen\n",
    "    writer.add_scalar(\"train_epoch/loss\", loss, epoch)\n",
    "    writer.add_scalar(\"train_epoch/class_loss\", class_loss, epoch)\n",
    "    writer.add_scalar(\"train_epoch/reg_loss\", reg_loss, epoch)\n",
    "    writer.add_scalar(\"train_epoch/accuracy\", accuracy, epoch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model = model.eval()\n",
    "\n",
    "        losses = []\n",
    "        for batch_data in tqdm(test_dataloader, total=len(test_dataloader)):\n",
    "            batch_data = batch_data.to(device)\n",
    "            this_batch_size = batch_data.batch.detach().max() + 1\n",
    "\n",
    "            pred_y, _, feature_transform = model(batch_data)\n",
    "            true_y = batch_data.y.detach()\n",
    "\n",
    "            class_loss = criteria(pred_y, true_y)\n",
    "            accuracy =float((pred_y.argmax(dim=1) == true_y).sum()) / float(this_batch_size)\n",
    "\n",
    "            id_matrix = torch.eye(feature_transform.shape[1]).to(feature_transform.device).view(1, 64, 64).repeat(feature_transform.shape[0], 1, 1)\n",
    "            transform_norm = torch.norm(torch.bmm(feature_transform, feature_transform.transpose(1, 2)) - id_matrix, dim=(1, 2))\n",
    "            reg_loss = transform_norm.mean()\n",
    "\n",
    "            loss = class_loss + reg_loss * 0.001\n",
    "\n",
    "            losses.append({\n",
    "                \"loss\": loss.item(),\n",
    "                \"class_loss\": class_loss.item(),\n",
    "                \"reg_loss\": reg_loss.item(),\n",
    "                \"accuracy\": accuracy,\n",
    "                \"seen\": float(this_batch_size)})\n",
    "            \n",
    "        loss = 0\n",
    "        class_loss = 0\n",
    "        reg_loss = 0\n",
    "        accuracy = 0\n",
    "        seen = 0\n",
    "        for d in losses:\n",
    "            seen = seen + d[\"seen\"]\n",
    "            loss = loss + d[\"loss\"] * d[\"seen\"]\n",
    "            class_loss = class_loss + d[\"class_loss\"] * d[\"seen\"]\n",
    "            reg_loss = reg_loss + d[\"reg_loss\"] * d[\"seen\"]\n",
    "            accuracy = accuracy + d[\"accuracy\"] * d[\"seen\"]\n",
    "        loss = loss / seen\n",
    "        class_loss = class_loss / seen\n",
    "        reg_loss = reg_loss / seen\n",
    "        accuracy = accuracy / seen\n",
    "        writer.add_scalar(\"test_epoch/loss\", loss, epoch)\n",
    "        writer.add_scalar(\"test_epoch/class_loss\", class_loss, epoch)\n",
    "        writer.add_scalar(\"test_epoch/reg_loss\", reg_loss, epoch)\n",
    "        writer.add_scalar(\"test_epoch/accuracy\", accuracy, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_data, y in tqdm(train_dataloader, total=len(train_dataloader)):\n",
    "    this_batch_size =  len(y)\n",
    "\n",
    "    batch_label = torch.zeros([this_batch_size*1024])\n",
    "    for i in range(this_batch_size):\n",
    "        batch_label[1024 * i : 1024 * (i + 1)] = i\n",
    "\n",
    "    batch_data = batch_data.reshape((1024 * this_batch_size,3)).to(device)\n",
    "    batch_label = batch_label.type(torch.LongTensor).to(device)\n",
    "    pred_y, _, feature_transform = model(batch_data, batch_label)\n",
    "    true_y = y.detach().to(device)\n",
    "\n",
    "    class_loss = criteria(pred_y, true_y)\n",
    "    accuracy = float((pred_y.argmax(dim=1) == true_y).sum()) / float(this_batch_size)\n",
    "\n",
    "    id_matrix = torch.eye(feature_transform.shape[1]).to(feature_transform.device).view(1, 64, 64).repeat(feature_transform.shape[0], 1, 1)\n",
    "    transform_norm = torch.norm(torch.bmm(feature_transform, feature_transform.transpose(1, 2)) - id_matrix, dim=(1, 2))\n",
    "    reg_loss = transform_norm.mean()\n",
    "\n",
    "    loss = class_loss + reg_loss * 0.001"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 18:24:45) \n[Clang 12.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "333f58ccc9c55b73347774f900429f7bf8931cea0ba5f13d1781f706049071fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
